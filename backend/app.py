import json
import logging
import os
import sqlite3
import time
import uuid
from datetime import datetime
from typing import Any, Dict, Union

from flask import Flask, jsonify, request
from flask_cors import CORS

try:
    from transformers import pipeline as hf_pipeline
    TRANSFORMERS_AVAILABLE = True
except Exception:
    TRANSFORMERS_AVAILABLE = False

# Try to import PostgreSQL driver
try:
    import psycopg2
    import psycopg2.extras

    POSTGRES_AVAILABLE = True
except ImportError:
    POSTGRES_AVAILABLE = False
    print("‚ö†Ô∏è  psycopg2 not available. PostgreSQL support disabled.")

app = Flask(__name__)
CORS(app)

# Database configuration
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///responses.db")
DATABASE = "responses.db"  # Fallback for SQLite

# Moderation configuration (local pipeline preferred)
MODERATION_THRESHOLD = 0.5

_moderation_pipe = None
if TRANSFORMERS_AVAILABLE:
    try:
        # Use binary toxic classifier; enable return_all_scores to read toxic score explicitly
        _moderation_pipe = hf_pipeline(
            "text-classification", model="unitary/toxic-bert", return_all_scores=True
        )
        logging.info("‚úÖ Loaded unitary/toxic-bert moderation pipeline")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Failed to load moderation pipeline, will use heuristic: {e}")
        _moderation_pipe = None


def _heuristic_toxicity(content: str) -> float:
    """Very lightweight heuristic toxicity score based on keyword hits.

    Returns score in [0,1]. This is a fallback when no API token is configured.
    """
    bad_terms = {
        "hate",
        "kill",
        "stupid",
        "idiot",
        "dumb",
        "trash",
        "racist",
        "sexist",
        "violent",
        "terror",
        "harass",
    }
    text = (content or "").lower()
    if not text:
        return 0.0
    hits = sum(1 for t in bad_terms if t in text)
    # Cap denominator to smooth overly short texts
    denom = max(5, len(text.split()))
    return min(1.0, hits / denom * 3)


def moderate_text(content: str) -> dict:
    """Run moderation and return dict with status ('SAFE'/'FLAGGED') and score.

    Uses local transformers pipeline when available, otherwise a simple heuristic.
    """
    score = 0.0
    details = {}

    if _moderation_pipe is not None:
        try:
            outputs = _moderation_pipe(content)
            # outputs is typically [[{label: 'toxic', score: x}, {label: 'non-toxic', score: y}]]
            if isinstance(outputs, list) and outputs and isinstance(outputs[0], list):
                for item in outputs[0]:
                    label = str(item.get("label", "")).lower()
                    s = float(item.get("score", 0.0))
                    details[label] = s
                score = details.get("toxic", 0.0)
            else:
                # Fallback parse for unexpected structure
                arr = outputs if isinstance(outputs, list) else []
                for item in arr:
                    label = str(item.get("label", "")).lower()
                    s = float(item.get("score", 0.0))
                    details[label] = s
                score = details.get("toxic", 0.0)
        except Exception as e:
            logging.warning(f"Moderation pipeline failed, using heuristic: {e}")
            score = _heuristic_toxicity(content)
    else:
        score = _heuristic_toxicity(content)

    status = "FLAGGED" if score >= MODERATION_THRESHOLD else "SAFE"
    return {"status": status, "score": score, "details": details}


def get_db_connection(max_retries: int = 5, base_delay: float = 1.0):
    """Create a database connection with automatic retry logic.

    Args:
        max_retries: Maximum number of connection attempts
        base_delay: Base delay between retries (exponential backoff)
    """
    db_url = os.getenv("DATABASE_URL", "sqlite:///responses.db")

    if db_url.startswith("postgresql://") or db_url.startswith("postgres://"):
        if not POSTGRES_AVAILABLE:
            raise ImportError("PostgreSQL URL provided but psycopg2 not installed")

        # PostgreSQL connection with retry logic
        for attempt in range(max_retries + 1):
            try:
                conn = psycopg2.connect(db_url)
                conn.autocommit = True

                # Test the connection
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.close()

                if attempt > 0:
                    logging.info(
                        f"‚úÖ PostgreSQL connection established after {attempt} retries"
                    )
                return conn

            except (psycopg2.OperationalError, psycopg2.DatabaseError) as e:
                if attempt == max_retries:
                    logging.error(
                        f"‚ùå Failed to connect to PostgreSQL after {max_retries} attempts: {e}"
                    )
                    raise

                delay = base_delay * (2**attempt)  # Exponential backoff
                logging.warning(
                    f"‚ö†Ô∏è  PostgreSQL connection attempt {attempt + 1} failed, retrying in {delay}s: {e}"
                )
                time.sleep(delay)
    else:
        # SQLite connection (default) - no retry needed for local files
        # Extract path from URL if it's a sqlite:// URL, otherwise use as-is
        if db_url.startswith("sqlite:///"):
            db_path = db_url[10:]  # Remove 'sqlite:///' prefix
        elif db_url.startswith("sqlite://"):
            db_path = db_url[9:]  # Remove 'sqlite://' prefix
        else:
            db_path = db_url

        # Ensure directory exists for SQLite
        os.makedirs(
            os.path.dirname(db_path) if os.path.dirname(db_path) else ".", exist_ok=True
        )

        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        return conn


def is_postgres():
    """Check if we're using PostgreSQL."""
    db_url = os.getenv("DATABASE_URL", "sqlite:///responses.db")
    return db_url.startswith("postgresql://") or db_url.startswith("postgres://")


def execute_query(conn, query: str, params: tuple = ()) -> Union[list, None]:
    """Execute a query with proper cursor handling for both SQLite and PostgreSQL."""
    if is_postgres():
        cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        cursor.execute(query, params)
        # Handle both SELECT and RETURNING clauses
        if query.strip().upper().startswith("SELECT") or "RETURNING" in query.upper():
            return cursor.fetchall()
        return None
    else:
        cursor = conn.execute(query, params)
        if query.strip().upper().startswith("SELECT"):
            return cursor.fetchall()
        conn.commit()
        return None


def init_db(max_retries: int = 10):
    """Initialize the database with required tables.

    Args:
        max_retries: Maximum number of initialization attempts
    """
    for attempt in range(max_retries + 1):
        try:
            conn = get_db_connection()

            if is_postgres():
                # PostgreSQL schema
                query = """
                    CREATE TABLE IF NOT EXISTS responses (
                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                        title VARCHAR(255) NOT NULL,
                        content TEXT NOT NULL,
                        tags JSONB DEFAULT '[]'::jsonb,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                    );
                """
            else:
                # SQLite schema
                query = """
                    CREATE TABLE IF NOT EXISTS responses (
                        id TEXT PRIMARY KEY,
                        title TEXT NOT NULL,
                        content TEXT NOT NULL,
                        tags TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """

            execute_query(conn, query)
            conn.close()

            db_type = "PostgreSQL" if is_postgres() else "SQLite"
            if attempt > 0:
                logging.info(
                    f"‚úÖ Database initialized ({db_type}) after {attempt} retries"
                )
            else:
                logging.info(f"‚úÖ Database initialized ({db_type})")
            return

        except Exception as e:
            if attempt == max_retries:
                logging.error(
                    f"‚ùå Failed to initialize database after {max_retries} attempts: {e}"
                )
                raise

            delay = 2**attempt  # Exponential backoff
            logging.warning(
                f"‚ö†Ô∏è  Database initialization attempt {attempt + 1} failed, retrying in {delay}s: {e}"
            )
            time.sleep(delay)


def dict_from_row(row) -> Dict[str, Any]:
    """Convert a database row to a dictionary."""
    if is_postgres():
        # PostgreSQL RealDictRow - tags is already JSONB (list/dict)
        tags = row["tags"] if row["tags"] is not None else []
        if isinstance(tags, str):
            tags = json.loads(tags)
    else:
        # SQLite Row - tags is JSON string
        tags = json.loads(row["tags"]) if row["tags"] else []

    return {
        "id": str(row["id"]),  # Ensure ID is always string for consistency
        "title": row["title"],
        "content": row["content"],
        "tags": tags,
        "created_at": str(row["created_at"]) if row["created_at"] else None,
        "updated_at": str(row["updated_at"]) if row["updated_at"] else None,
    }


@app.route("/api/responses", methods=["GET"])
def get_responses():
    """Get all responses, optionally filtered by search query."""
    search = request.args.get("search", "")

    conn = get_db_connection()

    if search:
        if is_postgres():
            # PostgreSQL with ILIKE for case-insensitive search and JSONB contains
            query = """
                SELECT * FROM responses
                WHERE title ILIKE %s OR content ILIKE %s OR tags::text ILIKE %s
                ORDER BY created_at DESC
            """
            search_term = f"%{search}%"
            rows = execute_query(conn, query, (search_term, search_term, search_term))
        else:
            # SQLite with LIKE
            query = """
                SELECT * FROM responses
                WHERE title LIKE ? OR content LIKE ? OR tags LIKE ?
                ORDER BY created_at DESC
            """
            search_term = f"%{search}%"
            rows = execute_query(conn, query, (search_term, search_term, search_term))
    else:
        rows = execute_query(conn, "SELECT * FROM responses ORDER BY created_at DESC")

    conn.close()

    responses = [dict_from_row(row) for row in rows] if rows else []
    return jsonify(responses)


@app.route("/api/responses/<response_id>", methods=["GET"])
def get_response(response_id: str):
    """Get a single response by ID."""
    conn = get_db_connection()

    if is_postgres():
        # PostgreSQL uses UUID type
        query = "SELECT * FROM responses WHERE id = %s"
        rows = execute_query(conn, query, (response_id,))
    else:
        # SQLite uses TEXT
        query = "SELECT * FROM responses WHERE id = ?"
        rows = execute_query(conn, query, (response_id,))

    conn.close()

    if not rows:
        return jsonify({"error": "Response not found"}), 404

    return jsonify(dict_from_row(rows[0]))


@app.route("/api/responses", methods=["POST"])
def create_response():
    """Create a new response."""
    data = request.get_json()

    if not data or "title" not in data or "content" not in data:
        return jsonify({"error": "Title and content are required"}), 400

    title = data["title"]
    content = data["content"]
    tags = data.get("tags", [])

    # Ensure tags is a list
    if not isinstance(tags, list):
        try:
            tags = json.loads(tags) if isinstance(tags, str) else []
        except Exception:
            tags = []

    # Run moderation and add status tag
    mod = moderate_text(content)
    # Remove any previous status tags just in case
    tags = [t for t in tags if str(t).upper() not in ("SAFE", "FLAGGED")]
    tags.append(mod["status"])  # Add SAFE/FLAGGED

    conn = get_db_connection()

    if is_postgres():
        # PostgreSQL with JSONB and auto-generated UUID
        query = """
            INSERT INTO responses (title, content, tags)
            VALUES (%s, %s, %s)
            RETURNING *
        """
        rows = execute_query(conn, query, (title, content, json.dumps(tags)))
        response_data = dict_from_row(rows[0]) if rows else None
    else:
        # SQLite with manual UUID
        response_id = str(uuid.uuid4())
        query = "INSERT INTO responses (id, title, content, tags) VALUES (?, ?, ?, ?)"
        execute_query(conn, query, (response_id, title, content, json.dumps(tags)))

        # Fetch the created record
        rows = execute_query(
            conn, "SELECT * FROM responses WHERE id = ?", (response_id,)
        )
        response_data = dict_from_row(rows[0]) if rows else None

    conn.close()

    if not response_data:
        return jsonify({"error": "Failed to create response"}), 500

    return jsonify(response_data), 201


@app.route("/api/responses/<response_id>", methods=["PUT", "PATCH"])
def update_response(response_id: str):
    """Update an existing response."""
    data = request.get_json()

    if not data:
        return jsonify({"error": "No data provided"}), 400

    conn = get_db_connection()

    # Check if response exists
    if is_postgres():
        check_query = "SELECT * FROM responses WHERE id = %s"
        check_params = (response_id,)
    else:
        check_query = "SELECT * FROM responses WHERE id = ?"
        check_params = (response_id,)

    existing = execute_query(conn, check_query, check_params)
    if not existing:
        conn.close()
        return jsonify({"error": "Response not found"}), 404

    # Build update query dynamically based on provided fields
    updates = []
    params = []

    if "title" in data:
        updates.append("title = %s" if is_postgres() else "title = ?")
        params.append(data["title"])

    tags_already_set = False

    if "content" in data:
        updates.append("content = %s" if is_postgres() else "content = ?")
        params.append(data["content"])

        # If content changes, recompute moderation status and update tags accordingly
        # Load existing tags from DB to update them
        existing_tags = []
        try:
            if is_postgres():
                existing_tags = existing[0]["tags"] if existing and existing[0]["tags"] else []
            else:
                existing_tags = json.loads(existing[0]["tags"]) if existing and existing[0]["tags"] else []
        except Exception:
            existing_tags = []

        if not isinstance(existing_tags, list):
            existing_tags = []

        # Recompute moderation
        mod = moderate_text(data["content"])
        new_tags = [t for t in existing_tags if str(t).upper() not in ("SAFE", "FLAGGED")]

        # If caller also provided tags, merge them (excluding SAFE/FLAGGED)
        if "tags" in data:
            incoming_tags = data["tags"]
            try:
                if not isinstance(incoming_tags, list):
                    incoming_tags = json.loads(incoming_tags) if isinstance(incoming_tags, str) else []
            except Exception:
                incoming_tags = []
            incoming_tags = [t for t in incoming_tags if str(t).upper() not in ("SAFE", "FLAGGED")]
            new_tags = new_tags + incoming_tags

        # Add SAFE/FLAGGED
        new_tags.append(mod["status"])  # Add SAFE/FLAGGED

        if is_postgres():
            updates.append("tags = %s::jsonb")
            params.append(json.dumps(new_tags))
        else:
            updates.append("tags = ?")
            params.append(json.dumps(new_tags))
        tags_already_set = True

    if "tags" in data and not tags_already_set:
        if is_postgres():
            updates.append("tags = %s::jsonb")
            params.append(json.dumps(data["tags"]))
        else:
            updates.append("tags = ?")
            params.append(json.dumps(data["tags"]))

    if updates:
        if is_postgres():
            updates.append("updated_at = CURRENT_TIMESTAMP")
            query = (
                f'UPDATE responses SET {", ".join(updates)} WHERE id = %s RETURNING *'
            )
            params.append(response_id)
            rows = execute_query(conn, query, params)
            response_data = dict_from_row(rows[0]) if rows else None
        else:
            updates.append("updated_at = CURRENT_TIMESTAMP")
            query = f'UPDATE responses SET {", ".join(updates)} WHERE id = ?'
            params.append(response_id)
            execute_query(conn, query, params)

            # Fetch updated record
            rows = execute_query(
                conn, "SELECT * FROM responses WHERE id = ?", (response_id,)
            )
            response_data = dict_from_row(rows[0]) if rows else None
    else:
        response_data = dict_from_row(existing[0])

    conn.close()

    return jsonify(response_data)


@app.route("/api/responses/<response_id>", methods=["DELETE"])
def delete_response(response_id: str):
    """Delete a response."""
    conn = get_db_connection()

    # Check if response exists
    if is_postgres():
        check_query = "SELECT * FROM responses WHERE id = %s"
        delete_query = "DELETE FROM responses WHERE id = %s"
        params = (response_id,)
    else:
        check_query = "SELECT * FROM responses WHERE id = ?"
        delete_query = "DELETE FROM responses WHERE id = ?"
        params = (response_id,)

    existing = execute_query(conn, check_query, params)
    if not existing:
        conn.close()
        return jsonify({"error": "Response not found"}), 404

    execute_query(conn, delete_query, params)
    conn.close()

    return "", 204


@app.route("/api/health", methods=["GET"])
def health_check():
    """Health check endpoint with database connectivity test."""
    try:
        # Test database connection
        conn = get_db_connection(max_retries=1)  # Quick test, don't wait long
        cursor = conn.cursor() if is_postgres() else conn

        if is_postgres():
            cursor.execute("SELECT 1")
        else:
            cursor.execute("SELECT 1")

        conn.close()

        return jsonify(
            {
                "status": "healthy",
                "timestamp": datetime.now().isoformat(),
                "database": "PostgreSQL" if is_postgres() else "SQLite",
                "database_connected": True,
            }
        )
    except Exception as e:
        return (
            jsonify(
                {
                    "status": "unhealthy",
                    "timestamp": datetime.now().isoformat(),
                    "database": "PostgreSQL" if is_postgres() else "SQLite",
                    "database_connected": False,
                    "error": str(e),
                }
            ),
            503,
        )


if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    # Show which database we're using
    db_url = os.getenv("DATABASE_URL", "sqlite:///responses.db")
    logging.info(f"üîß Using DATABASE_URL: {db_url}")

    try:
        # Initialize database with retry logic
        logging.info("üîÑ Initializing database...")
        init_db()

        logging.info("üöÄ Starting Flask server on http://0.0.0.0:5000")
        app.run(debug=True, host="0.0.0.0", port=5000)

    except Exception as e:
        logging.error(f"‚ùå Failed to start application: {e}")
        exit(1)
